% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/stats_functions.R
\name{multi.cv.glmnet}
\alias{multi.cv.glmnet}
\title{Repeated Cross-Validation of Sparse Logistic Models}
\usage{
multi.cv.glmnet(x, y, family = "binomial", eta = 0, nfolds = 10, nreps = 10)
}
\arguments{
\item{x}{a matrix with one row per sample and one column per feature}

\item{y}{a vector containing the true class assignments for all samples in x}

\item{family}{the model family to use. See [glmnet::glmnet()] for details.
Currently only supports binomial, multinomial, and gaussian.}

\item{eta}{the proportion of ridge regression to 'mix' into the LASSO
regression. A small amount (~0.05) can help improve model stability.}

\item{nfolds}{the number of cross-validation folds to use}

\item{nreps}{the number of times to repeat cross-validation}
}
\value{
A list with the following fields:
  \describe{
    \item{full_fit}{a glmnet model fit on all the data}
    \item{lambda_min}{the lambda value where deviance is minimized when
    averaged across all cross-validation folds.}
    \item{lambda_1se}{the lambda value giving the simplest model with mean
    deviance within 1 standard error of the lambda_min model.}
    \item{cv_folds}{the actual cross-validation training folds used.}
    \item{cv_fits}{the glmnet models fit on each training fold.}
    \item{lambda_perf}{a tibble containing mean and variation in model
    deviance when tested on the hold-out sets for each lambda value.}
  }
}
\description{
This function performs repeated k-fold cross-validation of sparse logistic
regression models. Unlike the vanilla cv.glmnet method, this version returns
the raw deviance values and selected features for all the models. This allows
proper selection of the optimal lambda value and the most common features.
Note that this method currently only optimizes on deviance (binomial models),
multinomial deviance (multinomial models), or absolute error (gaussian
models).
}
\seealso{
[glmnet::cv.glmnet()] for the original function on which this one
  was based.
}
